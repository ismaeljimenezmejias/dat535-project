{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69565998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "import csv\n",
    "import io\n",
    "import time\n",
    "import pandas as pd\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7254e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 0. Spark ===\n",
    "sc = SparkContext(appName=\"MentalHealthCleaning\")\n",
    "spark = SparkSession.builder.appName(\"MentalHealthCleaning\").getOrCreate()\n",
    "\n",
    "silver_path = \"gs://medallion-dat535/silver/mental_health_structured.csv\"\n",
    "gold_path = \"gs://medallion-dat535/gold\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c7a9e1",
   "metadata": {},
   "source": [
    "# RDD APROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0994631d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# === 1. RDD approach ===\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m start_rdd = \u001b[43mtime\u001b[49m.time()\n\u001b[32m      4\u001b[39m rdd = sc.textFile(silver_path)\n\u001b[32m      5\u001b[39m header = rdd.first()\n",
      "\u001b[31mNameError\u001b[39m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "# === 1. RDD approach ===\n",
    "start_rdd = time.time()\n",
    "\n",
    "rdd = sc.textFile(silver_path)\n",
    "header = rdd.first()\n",
    "columns = header.split(\",\")\n",
    "rdd_rows = rdd.filter(lambda x: x != header)\n",
    "\n",
    "def parse_line(line):\n",
    "    reader = csv.reader(io.StringIO(line))\n",
    "    row = next(reader)\n",
    "    return dict(zip(columns, row))\n",
    "\n",
    "rdd_dict = rdd_rows.map(parse_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14488db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for nulls\n",
    "\n",
    "def no_nulls(record):\n",
    "    return all(v not in (\"\", None, \"NULL\") for v in record.values())\n",
    "\n",
    "rdd_nonull = rdd_dict.filter(no_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9966727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarizing text fields\n",
    "def norm(record):\n",
    "    return {k: (v.title() if isinstance(v, str) else v) for k, v in record.items()}\n",
    "\n",
    "rdd_norm = rdd_nonull.map(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaa9488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging SocialWeaknessPrimary and SocialWeaknessSecondary if they are the same\n",
    "def merge_sw(r):\n",
    "    if (\"SocialWeaknessPrimary\" in r and\n",
    "        \"SocialWeaknessSecondary\" in r and\n",
    "        r[\"SocialWeaknessPrimary\"] == r[\"SocialWeaknessSecondary\"]):\n",
    "        r[\"SocialWeakness\"] = r[\"SocialWeaknessPrimary\"]\n",
    "        del r[\"SocialWeaknessPrimary\"]\n",
    "        del r[\"SocialWeaknessSecondary\"]\n",
    "    return r\n",
    "\n",
    "rdd_merged = rdd_norm.map(merge_sw)\n",
    "total_rows = rdd_merged.count()\n",
    "print(f\"RDD cleaned rows: {total_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467d8319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping metadata columns\n",
    "metadata_cols = [\"_ingestion_timestamp\", \"_source\", \"_status\", \"_raw_data\"]\n",
    "\n",
    "def drop_metadata(record):\n",
    "    return {k: v for k, v in record.items() if k not in metadata_cols}\n",
    "\n",
    "rdd_no_meta = rdd_merged.map(drop_metadata)\n",
    "\n",
    "df_rdd_clean = rdd_no_meta.toDF().repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c02c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the cleaned data in different formats\n",
    "\n",
    "# Guardar en GOLD con carpeta propia\n",
    "df_rdd_clean.write.mode(\"overwrite\").parquet(f\"{gold_path}/rdd/mental_health_clean.parquet\")\n",
    "df_rdd_clean.write.mode(\"overwrite\").option(\"header\", True).csv(f\"{gold_path}/rdd/mental_health_clean.csv\")\n",
    "df_rdd_clean.write.mode(\"overwrite\").json(f\"{gold_path}/rdd/mental_health_clean.json\")\n",
    "\n",
    "print(f\"✅ RDD cleaning completed in {time.time() - start_rdd:.2f} seconds.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227aad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking sizes\n",
    "from google.cloud import storage\n",
    "\n",
    "# Inicializa cliente GCS\n",
    "client = storage.Client()\n",
    "bucket_name = \"medallion-dat535\"\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "# Carpeta GOLD donde guardaste los resultados de RDD\n",
    "rdd_folders = [\n",
    "    \"gold/rdd/mental_health_clean.parquet/\",\n",
    "    \"gold/rdd/mental_health_clean.csv/\",\n",
    "    \"gold/rdd/mental_health_clean.json/\"\n",
    "]\n",
    "\n",
    "def get_folder_size(bucket, folder):\n",
    "    \"\"\"Devuelve tamaño total de todos los objetos en la carpeta en bytes.\"\"\"\n",
    "    total_size = 0\n",
    "    blobs = client.list_blobs(bucket, prefix=folder)\n",
    "    for blob in blobs:\n",
    "        total_size += blob.size\n",
    "    return total_size\n",
    "\n",
    "for folder in rdd_folders:\n",
    "    size_bytes = get_folder_size(bucket, folder)\n",
    "    size_mb = size_bytes / (1024*1024)\n",
    "    print(f\"{folder}: {size_bytes} bytes ({size_mb:.2f} MB)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deafb6b5",
   "metadata": {},
   "source": [
    "# Spark Dataframe Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7513fe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data again for Spark DataFrame approach\n",
    "start_df = time.time()\n",
    "\n",
    "df = spark.read.option(\"header\", True).csv(silver_path)\n",
    "\n",
    "metadata_cols = [\"_ingestion_timestamp\", \"_source\", \"_status\", \"_raw_data\"]\n",
    "existing_metadata_cols = [c for c in metadata_cols if c in df.columns]\n",
    "if existing_metadata_cols:\n",
    "    df = df.drop(*existing_metadata_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b16aa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for nulls\n",
    "for c in df.columns:\n",
    "    df = df.filter((col(c).isNotNull()) & (col(c) != \"\") & (col(c) != \"NULL\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accf149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarizing text fields\n",
    "text_cols = [f.name for f in df.schema.fields if str(f.dataType) == \"StringType\"]\n",
    "for c in text_cols:\n",
    "    df = df.withColumn(c, col(c).substr(1,1).upper() + col(c).substr(2, 1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a63ef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging SocialWeaknessPrimary and SocialWeaknessSecondary if they are the same\n",
    "if \"SocialWeaknessPrimary\" in df.columns and \"SocialWeaknessSecondary\" in df.columns:\n",
    "    df = df.withColumn(\n",
    "        \"SocialWeakness\",\n",
    "        when(col(\"SocialWeaknessPrimary\") == col(\"SocialWeaknessSecondary\"), col(\"SocialWeaknessPrimary\"))\n",
    "    ).drop(\"SocialWeaknessPrimary\", \"SocialWeaknessSecondary\")\n",
    "\n",
    "total_rows = df.count()\n",
    "print(f\"Spark DF cleaned rows: {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459bec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the cleaned data in different formats\n",
    "df.write.mode(\"overwrite\").parquet(f\"{gold_path}/spark_df/mental_health_clean.parquet\")\n",
    "df.write.mode(\"overwrite\").option(\"header\", True).csv(f\"{gold_path}/spark_df/mental_health_clean.csv\")\n",
    "df.write.mode(\"overwrite\").json(f\"{gold_path}/spark_df/mental_health_clean.json\")\n",
    "\n",
    "print(f\"✅ Spark DataFrame cleaning completed in {time.time() - start_df:.2f} seconds.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5230c22",
   "metadata": {},
   "source": [
    "# Pandas Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8519b169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data again for Pandas approach\n",
    "start_pd = time.time()\n",
    "\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(\"medallion-dat535\")\n",
    "blob = bucket.blob(\"silver/mental_health_structured.csv\")\n",
    "blob.download_to_filename(\"mental_health_structured.csv\")\n",
    "\n",
    "df_pd = pd.read_csv(\"mental_health_structured.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698ace26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for nulls\n",
    "df_pd_clean = df_pd.dropna()\n",
    "metadata_cols_pd = [\"_ingestion_timestamp\", \"_source\", \"_status\", \"_raw_data\"]\n",
    "df_pd_clean = df_pd_clean.drop(columns=[c for c in metadata_cols_pd if c in df_pd_clean.columns])\n",
    "\n",
    "text_cols_pd = df_pd_clean.select_dtypes(include=\"object\").columns\n",
    "df_pd_clean[text_cols_pd] = df_pd_clean[text_cols_pd].apply(lambda x: x.str.title())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781172d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging SocialWeaknessPrimary and SocialWeaknessSecondary if they are the same\n",
    "if \"SocialWeaknessSecondary\" in df_pd_clean.columns:\n",
    "    if (df_pd_clean[\"SocialWeaknessPrimary\"] == df_pd_clean[\"SocialWeaknessSecondary\"]).all():\n",
    "        df_pd_clean = df_pd_clean.drop(columns=[\"SocialWeaknessSecondary\"])\n",
    "        df_pd_clean = df_pd_clean.rename(columns={\"SocialWeaknessPrimary\":\"SocialWeakness\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caffb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the cleaned data in different formats\n",
    "# CSV \n",
    "df_pd_clean.to_csv(\"mental_health_clean_pandas.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Parquet\n",
    "df_pd_clean.to_parquet(\"mental_health_clean_pandas.parquet\", index=False)\n",
    "blob_parquet = bucket.blob(\"gold/pandas/mental_health_clean_pandas.parquet\")\n",
    "blob_parquet.upload_from_filename(\"mental_health_clean_pandas.parquet\")\n",
    "\n",
    "# JSON\n",
    "df_pd_clean.to_json(\"mental_health_clean_pandas.json\", orient=\"records\", lines=True)\n",
    "blob_json = bucket.blob(\"gold/pandas/mental_health_clean_pandas.json\")\n",
    "blob_json.upload_from_filename(\"mental_health_clean_pandas.json\")\n",
    "\n",
    "print(f\"✅ pandas cleaning completed in {time.time() - start_pd:.2f} seconds.\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
