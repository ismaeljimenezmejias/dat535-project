{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19a4d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922138cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Spark configuration\n",
    "# -----------------------------\n",
    "spark = SparkSession.builder.appName(\"UseCase1_RDD\").getOrCreate()\n",
    "\n",
    "df = spark.read.parquet(\"gs://medallion-dat535/gold/rdd/mental_health_clean.parquet\")\n",
    "df.createOrReplaceTempView(\"mental_health\")\n",
    "\n",
    "# Convert DF to RDD and cache it\n",
    "rdd = df.rdd.repartition(4).cache()\n",
    "# rdd = rdd.sample(False, 0.01, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d735bb3e",
   "metadata": {},
   "source": [
    "## RDD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58860704",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n========== USE CASE 1 (RDD ONLY) ==========\\n\")\n",
    "start = time.time()\n",
    "\n",
    "# ============================\n",
    "# A) Factor most associated with HIGH stress\n",
    "# ============================\n",
    "categorical_cols = [\n",
    "    \"Gender\", \"Country\", \"Occupation\", \"SelfEmployed\", \"FamilyHistory\",\n",
    "    \"Treatment\", \"DaysIndoors\", \"HabitsChange\", \"MentalHealthHistory\",\n",
    "    \"MoodSwings\", \"SocialWeakness\", \"CopingStruggles\", \"WorkInterest\",\n",
    "    \"MentalHealthInterview\", \"CareOptions\"\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    # (category_value, isHighStress)\n",
    "    pairs = rdd.map(lambda row: (\n",
    "        row[col],\n",
    "        1 if row[\"IncreasingStress\"].strip().lower() == \"yes\" else 0\n",
    "    )).filter(lambda x: x[0] is not None)  # remove nulls\n",
    "\n",
    "    # (category_value, (sumHigh, count))\n",
    "    stats = pairs.aggregateByKey(\n",
    "        (0, 0),\n",
    "        lambda acc, value: (acc[0] + value, acc[1] + 1),\n",
    "        lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
    "    )\n",
    "\n",
    "    # proportion of \"High\" by category\n",
    "    proportions = stats.mapValues(lambda x: x[0] / x[1] if x[1] > 0 else 0)\n",
    "    # category with highest High Stress proportion\n",
    "    best = proportions.reduce(lambda a, b: a if a[1] > b[1] else b)\n",
    "    results[col] = best\n",
    "\n",
    "print(\"\\n=== Factor most associated with HIGH stress ===\")\n",
    "for col, (category, prop) in results.items():\n",
    "    print(f\"{col}: '{category}' â†’ {prop:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f7d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for debugging\n",
    "\n",
    "# # Check the actual values\n",
    "# vals = rdd.map(lambda r: r[\"IncreasingStress\"]).distinct().collect()\n",
    "# print(\"Unique values of IncreasingStress:\", vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4b524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# B) Stress distribution by country\n",
    "# ============================\n",
    "country_pairs = rdd.map(lambda r: (r[\"Country\"], r[\"IncreasingStress\"])).filter(lambda x: x[0] is not None)\n",
    "\n",
    "def count_categories(acc, v):\n",
    "    acc[v] = acc.get(v, 0) + 1\n",
    "    return acc\n",
    "\n",
    "def merge_dicts(d1, d2):\n",
    "    for k, v in d2.items():\n",
    "        d1[k] = d1.get(k, 0) + v\n",
    "    return d1\n",
    "\n",
    "country_stats = country_pairs.aggregateByKey({}, count_categories, merge_dicts).collect()\n",
    "\n",
    "print(\"\\n=== Stress distribution by country ===\")\n",
    "for country, stats in country_stats:\n",
    "    total = sum(stats.values())\n",
    "    proportions = {k: v / total for k, v in stats.items()}\n",
    "    print(f\"{country}: {proportions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b2b2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# C) Global distribution of IncreasingStress\n",
    "# ============================\n",
    "stress_counts = rdd.map(lambda r: (r[\"IncreasingStress\"], 1)) \\\n",
    "                   .filter(lambda x: x[0] is not None) \\\n",
    "                   .reduceByKey(lambda a, b: a + b) \\\n",
    "                   .collect()\n",
    "total = sum([c for _, c in stress_counts])\n",
    "print(\"\\n=== Global distribution of IncreasingStress ===\")\n",
    "for k, v in stress_counts:\n",
    "    print(f\"{k}: {v} ({v/total:.2%})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb899544",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# D) People with mental health history by country\n",
    "# ============================\n",
    "mh_history = rdd.map(lambda r: (r[\"Country\"], 1 if r[\"MentalHealthHistory\"] and r[\"MentalHealthHistory\"].strip().lower() == \"yes\" else 0)) \\\n",
    "                .filter(lambda x: x[0] is not None)\n",
    "mh_by_country = mh_history.aggregateByKey((0,0), \n",
    "                                          lambda acc, v: (acc[0]+v, acc[1]+1), \n",
    "                                          lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) \\\n",
    "                    .collect()\n",
    "print(\"\\n=== Proportion of people with mental health history by country ===\")\n",
    "for country, (yes_count, total_count) in mh_by_country:\n",
    "    print(f\"{country}: {yes_count}/{total_count} ({yes_count/total_count:.2%})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf552d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================\n",
    "# E) Proportion of SelfEmployed by Occupation\n",
    "# ============================\n",
    "selfemp_pairs = rdd.map(lambda r: (r[\"Occupation\"], 1 if r[\"SelfEmployed\"] and r[\"SelfEmployed\"].strip().lower() == \"yes\" else 0)) \\\n",
    "                   .filter(lambda x: x[0] is not None)\n",
    "selfemp_stats = selfemp_pairs.aggregateByKey((0,0), \n",
    "                                             lambda acc, v: (acc[0]+v, acc[1]+1), \n",
    "                                             lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) \\\n",
    "                         .collect()\n",
    "print(\"\\n=== Proportion of SelfEmployed by Occupation ===\")\n",
    "for occ, (yes_count, total_count) in selfemp_stats:\n",
    "    print(f\"{occ}: {yes_count}/{total_count} ({yes_count/total_count:.2%})\")\n",
    "\n",
    "print(f\"\\nRDD completed in {time.time() - start:.2f} seconds.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355e9392",
   "metadata": {},
   "source": [
    "## DATAFRAME API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd5787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_df = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b305161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A) Factor most associated with HIGH stress\n",
    "df_results = {}\n",
    "for col in categorical_cols:\n",
    "    prop_df = df.filter(F.col(col).isNotNull()) \\\n",
    "                .withColumn(\"isHigh\", F.when(F.lower(F.col(\"IncreasingStress\"))==\"yes\",1).otherwise(0)) \\\n",
    "                .groupBy(col) \\\n",
    "                .agg(F.avg(\"isHigh\").alias(\"prop\")) \\\n",
    "                .orderBy(F.desc(\"prop\")) \\\n",
    "                .limit(1) \\\n",
    "                .collect()\n",
    "    df_results[col] = (prop_df[0][col], prop_df[0][\"prop\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55a6a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# B) Stress distribution by country\n",
    "df_country = df.groupBy(\"Country\", \"IncreasingStress\") \\\n",
    "               .count() \\\n",
    "               .withColumn(\"prop\", F.col(\"count\") / F.sum(\"count\").over(Window.partitionBy(\"Country\"))) \\\n",
    "               .orderBy(\"Country\", \"IncreasingStress\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e89f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# C) Global distribution of IncreasingStress\n",
    "df_stress_global = df.groupBy(\"IncreasingStress\") \\\n",
    "                     .count() \\\n",
    "                     .withColumn(\"prop\", F.col(\"count\")/F.sum(\"count\").over(Window.partitionBy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd1f60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# D) People with mental health history by country\n",
    "df_mh = df.filter(F.col(\"MentalHealthHistory\").isNotNull()) \\\n",
    "          .withColumn(\"hasHistory\", F.when(F.lower(F.col(\"MentalHealthHistory\"))==\"yes\",1).otherwise(0)) \\\n",
    "          .groupBy(\"Country\") \\\n",
    "          .agg(F.sum(\"hasHistory\").alias(\"yes_count\"), F.count(\"*\").alias(\"total_count\")) \\\n",
    "          .withColumn(\"prop\", F.col(\"yes_count\")/F.col(\"total_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead7d896",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# E) Proportion of SelfEmployed by Occupation\n",
    "df_selfemp = df.filter(F.col(\"SelfEmployed\").isNotNull() & F.col(\"Occupation\").isNotNull()) \\\n",
    "               .withColumn(\"isSelf\", F.when(F.lower(F.col(\"SelfEmployed\"))==\"yes\",1).otherwise(0)) \\\n",
    "               .groupBy(\"Occupation\") \\\n",
    "               .agg(F.sum(\"isSelf\").alias(\"yes_count\"), F.count(\"*\").alias(\"total_count\")) \\\n",
    "               .withColumn(\"prop\", F.col(\"yes_count\")/F.col(\"total_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31c81e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "time_df = time.time() - start_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb70c06a",
   "metadata": {},
   "source": [
    "## SQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b29250",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_sql = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afeb3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A) Factor most associated with HIGH stress\n",
    "sql_results = {}\n",
    "for col in categorical_cols:\n",
    "    query = f\"\"\"\n",
    "    SELECT {col}, AVG(CASE WHEN LOWER(IncreasingStress)='yes' THEN 1 ELSE 0 END) AS prop\n",
    "    FROM mental_health\n",
    "    WHERE {col} IS NOT NULL\n",
    "    GROUP BY {col}\n",
    "    ORDER BY prop DESC\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    row = spark.sql(query).collect()[0]\n",
    "    sql_results[col] = (row[col], row['prop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5e012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# B) Stress distribution by country\n",
    "sql_country = spark.sql(\"\"\"\n",
    "    SELECT Country, IncreasingStress, COUNT(*) AS count,\n",
    "           COUNT(*) * 1.0 / SUM(COUNT(*)) OVER(PARTITION BY Country) AS prop\n",
    "    FROM mental_health\n",
    "    GROUP BY Country, IncreasingStress\n",
    "    ORDER BY Country, IncreasingStress\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c0a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# C) Global distribution of IncreasingStress\n",
    "sql_stress_global = spark.sql(\"\"\"\n",
    "    SELECT IncreasingStress, COUNT(*) AS count,\n",
    "           COUNT(*) * 1.0 / SUM(COUNT(*)) OVER() AS prop\n",
    "    FROM mental_health\n",
    "    GROUP BY IncreasingStress\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34037fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# D) People with mental health history by country\n",
    "sql_mh = spark.sql(\"\"\"\n",
    "    SELECT Country,\n",
    "           SUM(CASE WHEN LOWER(MentalHealthHistory)='yes' THEN 1 ELSE 0 END) AS yes_count,\n",
    "           COUNT(*) AS total_count,\n",
    "           SUM(CASE WHEN LOWER(MentalHealthHistory)='yes' THEN 1 ELSE 0 END) * 1.0 / COUNT(*) AS prop\n",
    "    FROM mental_health\n",
    "    WHERE MentalHealthHistory IS NOT NULL\n",
    "    GROUP BY Country\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44507478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E) Proportion of SelfEmployed by Occupation\n",
    "sql_selfemp = spark.sql(\"\"\"\n",
    "    SELECT Occupation,\n",
    "           SUM(CASE WHEN LOWER(SelfEmployed)='yes' THEN 1 ELSE 0 END) AS yes_count,\n",
    "           COUNT(*) AS total_count,\n",
    "           SUM(CASE WHEN LOWER(SelfEmployed)='yes' THEN 1 ELSE 0 END) * 1.0 / COUNT(*) AS prop\n",
    "    FROM mental_health\n",
    "    WHERE SelfEmployed IS NOT NULL AND Occupation IS NOT NULL\n",
    "    GROUP BY Occupation\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bdb76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "time_sql = time.time() - start_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6358aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mostrar todas las vistas temporales registradas\n",
    "spark.catalog.listTables()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be6834e",
   "metadata": {},
   "source": [
    "# Execution times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc813482",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n=== Execution times (seconds) ===\")\n",
    "print(f\"RDD total time: {time.time() - start:.2f}\")\n",
    "print(f\"DataFrame API total time: {time_df:.2f}\")\n",
    "print(f\"SQL total time: {time_sql:.2f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
